{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据挖掘第二次作业——实现一个分类器\n",
    "## 作业要求\n",
    "> 请自己编程实现一个分类器（如决策树、朴素贝叶斯、基于规则的分类器等），自行确定实验数据集，在数据集上与商用系统中的同类分类器在不同指标上开展性能对比，并用t检验确定你的分类器和对比的分类器的性能差异是否显著？\n",
    "## 实验内容\n",
    "本次实验选择了朴素贝叶斯作为学习和实现的对象，有关朴素贝叶斯的理论知识可以参考李航的《统计学习方法》和浙江大学胡浩基老师的《机器学习》课程。\n",
    "## 数据集\n",
    "本次实验选择了经典的breast_cancer数据集，使用sklearn提供的接口；同样的，也可以自行在[UCI](https://archive.ics.uci.edu/ml/datasets/)的网站上下载。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 朴素贝叶斯简介\n",
    "\n",
    "## 简介\n",
    "朴素贝叶斯法是机遇贝叶斯定理与特征条件独立假设的分类方法。\n",
    "对于给定的训练集，首先机遇特征条件独立假设输入输出的联合概率分布；然后根据此模型，对于给定的输入x，利用贝叶斯定理求出后验概率的最大输出的y。\n",
    "在了解朴素贝叶斯之前，我们首先需要了解贝叶斯定理。\n",
    "### 贝叶斯定理\n",
    "$$P(A_{i}|B)=\\frac {P(B|A_{i})P(A_{i})}{\\sum _{j}P(B|A_{i})P(A_{i})}$$\n",
    "\n",
    "## 4.1 朴素贝叶斯的学习与分类\n",
    "### 4.1.1 基本方法\n",
    "训练数据集T。\n",
    "朴素贝叶斯通过训练数据计算先验概率分布和条件概率分布来学习联合概率分布函数。\n",
    "这边直接给出最终的表示形式\n",
    "$$y=\\arg \\max _{c_k} P(Y=c_k)\\prod _j P(X^{(j)}=x^{(j)}|Y=c_k)$$\n",
    "\n",
    "\n",
    "## 4.2 朴素贝叶斯的参数估计\n",
    "### 4.2.1 极大似然估计\n",
    "这一部分略去。\n",
    "\n",
    "### 学习与分类算法\n",
    "输入：训练数据T\n",
    "输出：实例x的分类\n",
    "\n",
    "1. 计算现眼吗概率和条件概率\n",
    "2. 对于给定的实例x，计算：\n",
    "$$P(Y=c_k)\\prod _{j=1} ^{n} P(X^{(j)}=x^{(j)}|Y=c_k)$$\n",
    "3. 确定x的类为：使得上述值最大的y。\n",
    "\n",
    "## 高斯朴素贝叶斯\n",
    "由于本次实验的数据为连续值，所以我们使用高斯朴素贝叶斯来实现分类操作。\n",
    "一般使用的模型还有：\n",
    "1. 多项式模型\n",
    "2. 伯努利模型\n",
    "\n",
    "feature的可能性被假设为高斯函数：\n",
    "$$\\begin{equation}\n",
    "P\\left(x_{i} \\mid y_{k}\\right)=\\frac{1}{\\sqrt{2 \\pi \\sigma_{y k}^{2}}} \\exp \\left(-\\frac{\\left(x_{i}-\\mu_{y k}\\right)^{2}}{2 \\sigma_{y k}^{2}}\\right)\n",
    "\\end{equation}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# iris dataset的导入\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "# 训练/测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集初始化\n",
    "def load_dataset():\n",
    "    # return X, y(np.array)\n",
    "    cancer = load_breast_cancer()\n",
    "    data = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "    data['label'] = cancer.target\n",
    "    data = np.asarray(data)\n",
    "    return data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_dataset()\n",
    "# 训练集和测试集分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯分类器的实现\n",
    "class Bayes:\n",
    "    def __init__(self):\n",
    "        self.core = None\n",
    "    \n",
    "    # 期望\n",
    "    def _E(self, X):\n",
    "        return sum(X) / float(len(X))\n",
    "    # 标准差\n",
    "    def _std(self, X):\n",
    "        return np.std(X)\n",
    "\n",
    "    # 高斯函数计算\n",
    "    def _gauss(self, x, E, std):\n",
    "        exp = math.exp(-(math.pow(x - E, 2) / (2 * math.pow(std, 2))))\n",
    "        return (1 / (math.sqrt(2 * math.pi) * std)) * exp\n",
    "    \n",
    "    # 训练数据的处理\n",
    "    def _preprocess(self, data):\n",
    "        # return [(E, std)***]\n",
    "        # feed for the _gauss() function\n",
    "        return [(self._E(i), self._std(i)) for i in zip(*data)]\n",
    "    \n",
    "    # 训练模型：求出每个标签的数学期望和标准差\n",
    "    def fit(self, X, y):\n",
    "        labels = list(set(y))\n",
    "        data = {l: [] for l in labels}\n",
    "        for arr, l in zip(X, y):\n",
    "            data[l].append(arr)\n",
    "        self.core = {\n",
    "            l: self._preprocess(v)\n",
    "            for l, v in data.items()\n",
    "        }\n",
    "\n",
    "    # 计算概率\n",
    "    def _calculate(self, data):\n",
    "        prob = {}\n",
    "        for label, values in self.core.items():\n",
    "            temp = 1\n",
    "            for i, value in enumerate(values):\n",
    "                E = value[0]\n",
    "                std = value[1]\n",
    "                temp *= self._gauss(data[i], E, std)\n",
    "            prob[label] = temp\n",
    "        return prob\n",
    "\n",
    "\n",
    "        \n",
    "    # 预测函数\n",
    "    def predict(self, X):\n",
    "        # 返回概率最大的label\n",
    "        return sorted(self._calculate(X).items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "\n",
    "    # 评价函数: 计算accu\n",
    "    def score(self, X, y):\n",
    "        count = 0\n",
    "        for x, label in zip(X, y):\n",
    "            y_pred = self.predict(x)\n",
    "            if y_pred == label:\n",
    "                count += 1\n",
    "        return float(count)/float(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9210526315789473"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accu\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn 高斯贝叶斯\n",
    "接下来部分，我们展现sklearn中实现的高斯贝叶斯分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accu\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 性能检验\n",
    "在此部分我们使用了accuracy, binary-F1在进行测试，均进行25次测试，使用`scipy.stats`来进行t检验。\n",
    "我们在25次测试中，通过设置`train_test_split`中`random_state`的值为`None`来确保每次生成的训练/测试数据不同。\n",
    "\n",
    "此处的F1指的是：F1 = 2 * (precision * recall) / (precision + recall), 是相对于precision和recall而言更加综合的指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9210526315789473\n"
     ]
    }
   ],
   "source": [
    "# accuracy测试\n",
    "\n",
    "myAccu = []\n",
    "for _ in range(1):\n",
    "    myModel = Bayes()\n",
    "    myModel.fit(X_train, y_train)\n",
    "    print(myModel.score(X_test, y_test))\n",
    "\n",
    "skAccu = []\n",
    "for _ in range(25):\n",
    "    skModel = GaussianNB()\n",
    "    skModel.fit(X_train, y_train)\n",
    "    skAccu.append(skModel.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "myAccu = []\n",
    "myF1 = []\n",
    "skAccu = []\n",
    "skF1 = []\n",
    "for _ in range(25):\n",
    "    # my model\n",
    "    tx_train, tx_test, ty_train, ty_test = train_test_split(X, y, random_state=None)\n",
    "    myModel = Bayes()\n",
    "    myModel.fit(tx_train, ty_train)\n",
    "    myPred = [myModel.predict(x) for x in tx_test]\n",
    "    myAccu.append(myModel.score(tx_test, ty_test))\n",
    "    myF1.append(f1_score(ty_test, myPred))\n",
    "\n",
    "    # sklearn\n",
    "    skModel = GaussianNB()\n",
    "    skModel.fit(tx_train, ty_train)\n",
    "    skPred = skModel.predict(tx_test)\n",
    "    skAccu.append(skModel.score(tx_test, ty_test))\n",
    "    skF1.append(f1_score(ty_test, skPred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My F1 score: 0.948714020512844\n",
      "My Accu is: 0.9348251748251749\n",
      "SK F1 score: 0.953743031963181\n",
      "SK Accu is: 0.9406993006993009\n"
     ]
    }
   ],
   "source": [
    "print(\"My F1 score: \"+str(np.mean(myF1)))\n",
    "print(\"My Accu is: \"+str(np.mean(myAccu)))\n",
    "\n",
    "print(\"SK F1 score: \"+str(np.mean(skF1)))\n",
    "print(\"SK Accu is: \"+str(np.mean(skAccu)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可以看出，无论是精确度还是F1，sklearn提供的高斯朴素贝叶斯的效果都是比我自己实现的要更加好。\n",
    "### t-检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=-4.772492343041904, pvalue=7.405689255925076e-05)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 进行t-检验\n",
    "from scipy import stats\n",
    "stats.ttest_rel(myF1, skF1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原假设：\n",
    "H0:μ=μ0 \n",
    "H1:μ≠μ0\n",
    "所以拒绝原假设。\n",
    "\n",
    "所以总体而言的话sklearn自带的高斯朴素贝叶斯的整体性能高于我自己实现的模型。"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c68a72f625d73eaf66b9a93536cdd7f6c501fd47d973718bb3bc9dfb35acf95c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('venv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}